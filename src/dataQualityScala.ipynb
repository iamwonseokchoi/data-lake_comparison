{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c13b82",
   "metadata": {},
   "source": [
    "# Data Quality Mini-Workshop\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d079d",
   "metadata": {},
   "source": [
    "- Simple mini workshop working with Scala directly on a Jupyter noteeok using Spark\n",
    "- Simple tests and code to check for data quality\n",
    "\n",
    "**To use Scala Spark directly, we use `spylon-kernel` which can be abstracted onto your python environment:** \n",
    "```\n",
    "pip install spylon-kernel\n",
    "python -m spylon_kernel install --user\n",
    "```\n",
    "Then launch Jupyter and select the spylon-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c912b23-8a86-4669-9fb3-044c7bb9a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.35.161:4041\n",
       "SparkContext available as 'sc' (version = 3.4.1, master = local[*], app id = local-1697816975153)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@c4445ff\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b4f96a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "78e40efe-06f6-4a3f-ac2c-ab3fe2f4bbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true),StructField(name,StringType,true),StructField(age,IntegerType,true),StructField(salary,IntegerType,true),StructField(bonus,IntegerType,true),StructField(compTotal,IntegerType,true),StructField(registered,TimestampType,true))\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define schema \n",
    "val schema = StructType(Array(\n",
    "    StructField(\"id\", IntegerType, true),\n",
    "    StructField(\"name\", StringType, true),\n",
    "    StructField(\"age\", IntegerType, true),\n",
    "    StructField(\"salary\", IntegerType, true),\n",
    "    StructField(\"bonus\", IntegerType, true),\n",
    "    StructField(\"compTotal\", IntegerType, true),\n",
    "    StructField(\"registered\", TimestampType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc140330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp: java.sql.Timestamp = 2023-10-21 08:35:35.789\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[249] at parallelize at <console>:40\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// RDD of rows\n",
    "val timestamp = new java.sql.Timestamp(System.currentTimeMillis())\n",
    "val rdd = spark.sparkContext.parallelize(Array(\n",
    "    Row(1, \"John\", 21, 1000, 500, 1500, timestamp),\n",
    "    Row(2, \"Smith\", 23, 2000, 200, 2200, timestamp),\n",
    "    Row(3, \"Mary\", 22, 3000, 1000, 4000, timestamp),\n",
    "    Row(4, \"Jane\", 24, 4000, 0, 5000, timestamp),\n",
    "    Row(5, \"Joe\", 25, 5000, 100, 5100, timestamp),\n",
    "    Row(5, \"Joe\", 25, 5000, 300, 5500, timestamp),\n",
    "    Row(7, \"Adam\", null, null, null, null, null),\n",
    "    Row(8, null, null, null, null, null, null)\n",
    ")) // Duplicate rows for cardinality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "079d5bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+------+-----+---------+--------------------+\n",
      "| id| name| age|salary|bonus|compTotal|          registered|\n",
      "+---+-----+----+------+-----+---------+--------------------+\n",
      "|  1| John|  21|  1000|  500|     1500|2023-10-21 08:35:...|\n",
      "|  2|Smith|  23|  2000|  200|     2200|2023-10-21 08:35:...|\n",
      "|  3| Mary|  22|  3000| 1000|     4000|2023-10-21 08:35:...|\n",
      "|  4| Jane|  24|  4000|    0|     5000|2023-10-21 08:35:...|\n",
      "|  5|  Joe|  25|  5000|  100|     5100|2023-10-21 08:35:...|\n",
      "|  5|  Joe|  25|  5000|  300|     5500|2023-10-21 08:35:...|\n",
      "|  7| Adam|null|  null| null|     null|                null|\n",
      "|  8| null|null|  null| null|     null|                null|\n",
      "+---+-----+----+------+-----+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create DataFrame\n",
    "val df = spark.createDataFrame(rdd, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9007e8",
   "metadata": {},
   "source": [
    "### Null Value Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7169a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|age|salary|\n",
      "+---+----+---+------+\n",
      "|  0|   1|  2|     2|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nullCheck: org.apache.spark.sql.DataFrame = [id: bigint, name: bigint ... 2 more fields]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Identify columns with nulls\n",
    "val nullCheck = df.select(df.columns.map(c => \n",
    "    sum(when(col(c).isNull || isnan(col(c)), 1).otherwise(0)).alias(c)\n",
    "): _*)\n",
    "\n",
    "// Python\n",
    "// null_check = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "nullCheck.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c62d8",
   "metadata": {},
   "source": [
    "### Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a674e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|duplicates|\n",
      "+----------+\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "duplicates: org.apache.spark.sql.DataFrame = [duplicates: bigint]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Identify if duplicates exist\n",
    "val duplicates = df.groupBy(df.columns.map(col): _*)\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    "    .agg(sum(\"count\").alias(\"duplicates\"))\n",
    "\n",
    "// Python\n",
    "// duplicates = df.groupBy(df.columns).count().where(col(\"count\") > 1).select(sum(\"count\").alias(\"duplicates\")\n",
    "\n",
    "duplicates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6f005",
   "metadata": {},
   "source": [
    "### Unique Key Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bdbeb2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  5|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "uniqueKeyCheck: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, count: bigint]\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Ensure designated unique key columns are indeed unique\n",
    "val uniqueKeyCheck = df.groupBy(\"id\")\n",
    "    .count()\n",
    "    .where(col(\"count\") > 1)\n",
    "\n",
    "// Python\n",
    "// unique_key_check = df.groupBy(\"id\").count().where(col(\"count\") > 1)\n",
    "\n",
    "uniqueKeyCheck.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353b1ce",
   "metadata": {},
   "source": [
    "### Data Type Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98d2d5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatches in id: 0\n",
      "Mismatches in name: 0\n",
      "Mismatches in age: 0\n",
      "Mismatches in salary: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataTypeCheck: scala.collection.immutable.Map[String,org.apache.spark.sql.types.AtomicType with Product with Serializable] = Map(id -> IntegerType, name -> StringType, age -> IntegerType, salary -> IntegerType)\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Check that each column conforms to expected data type\n",
    "val dataTypeCheck = Map(\n",
    "    \"id\" -> IntegerType,\n",
    "    \"name\" -> StringType,\n",
    "    \"age\" -> IntegerType,\n",
    "    \"salary\" -> IntegerType\n",
    ")\n",
    "dataTypeCheck.foreach { case (column, dataType) =>\n",
    "    val mismatches = df.filter(col(column).cast(dataType) =!= col(column))\n",
    "        .count()\n",
    "    println(s\"Mismatches in $column: $mismatches\")\n",
    "}\n",
    "\n",
    "// Python\n",
    "// data_type_check = {\n",
    "//     \"id\": IntegerType,\n",
    "//     \"name\": StringType,\n",
    "//     \"age\": IntegerType,\n",
    "//     \"salary\": IntegerType\n",
    "// }\n",
    "// for column, data_type in data_type_check.items():\n",
    "//     mismatches = df.filter(col(column).cast(data_type) != col(column)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32685dec",
   "metadata": {},
   "source": [
    "### Value Range Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd15ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|age|salary|\n",
      "+---+----+---+------+\n",
      "|  4|Jane| 24|  4000|\n",
      "|  5| Joe| 25|  5000|\n",
      "|  5| Joe| 25|  5000|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lowerBound: Int = 20\n",
       "upperBound: Int = 23\n",
       "rangeCheck: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Check that numeric columns fall within expected ranges\n",
    "val lowerBound = 20\n",
    "val upperBound = 23\n",
    "val rangeCheck = df.filter((col(\"age\") < lowerBound) || (col(\"age\") > upperBound))\n",
    "\n",
    "// Python\n",
    "// lower_bound = 20\n",
    "// upper_bound = 23\n",
    "// range_check = df.filter((col(\"age\") < lower_bound) | (col(\"age\") > upper_bound))\n",
    "\n",
    "rangeCheck.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e084d4",
   "metadata": {},
   "source": [
    "### Categorical Value Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2707d034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+------+\n",
      "| id| name| age|salary|\n",
      "+---+-----+----+------+\n",
      "|  2|Smith|  23|  2000|\n",
      "|  4| Jane|  24|  4000|\n",
      "|  5|  Joe|  25|  5000|\n",
      "|  5|  Joe|  25|  5000|\n",
      "|  7| Adam|null|  null|\n",
      "+---+-----+----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "validCategories: Seq[String] = List(Mary, John, Relu)\n",
       "categoryCheck: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Ensure that categorical columns only contain expected values\n",
    "val validCategories = Seq(\"Mary\", \"John\", \"Relu\")\n",
    "val categoryCheck = df.filter(!col(\"name\").isin(validCategories: _*))\n",
    "\n",
    "// Python\n",
    "// valid_categories = [\"Tanh\", \"Sigmoid\", \"Relu\"]\n",
    "// category_check = df.filter(~col(\"name\").isin(valid_categories))\n",
    "\n",
    "categoryCheck.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a25c4",
   "metadata": {},
   "source": [
    "### Date Range Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ad6db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+--------------------+\n",
      "| id| name|age|salary|          registered|\n",
      "+---+-----+---+------+--------------------+\n",
      "|  1| John| 21|  1000|2023-10-21 08:29:...|\n",
      "|  2|Smith| 23|  2000|2023-10-21 08:29:...|\n",
      "|  3| Mary| 22|  3000|2023-10-21 08:29:...|\n",
      "|  4| Jane| 24|  4000|2023-10-21 08:29:...|\n",
      "|  5|  Joe| 25|  5000|2023-10-21 08:29:...|\n",
      "|  5|  Joe| 25|  5000|2023-10-21 08:29:...|\n",
      "+---+-----+---+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lowerDate: String = 2019-01-01\n",
       "upperDate: String = 2022-12-31\n",
       "dateCheck: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Check that timestamp columns fall within expected ranges\n",
    "val lowerDate = \"2019-01-01\"\n",
    "val upperDate = \"2022-12-31\"\n",
    "val dateCheck = df.filter((col(\"registered\") < lowerDate) || (col(\"registered\") > upperDate))\n",
    "\n",
    "// Python\n",
    "// lower_date = \"2019-01-01\"\n",
    "// upper_date = \"2022-12-31\"\n",
    "// date_check = df.filter((col(\"registered\") < lower_date) | (col(\"registered\") > upper_date))\n",
    "\n",
    "dateCheck.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82af610f",
   "metadata": {},
   "source": [
    "### Consistency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3a143c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+-----+---------+--------------------+\n",
      "| id|name|age|salary|bonus|compTotal|          registered|\n",
      "+---+----+---+------+-----+---------+--------------------+\n",
      "|  4|Jane| 24|  4000|    0|     5000|2023-10-21 08:35:...|\n",
      "|  5| Joe| 25|  5000|  300|     5500|2023-10-21 08:35:...|\n",
      "+---+----+---+------+-----+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "consistencyCheck: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Check if data in different columns are consistent with each other\n",
    "val consistencyCheck = df.filter(col(\"salary\") + col(\"bonus\") =!= col(\"compTotal\"))\n",
    "\n",
    "// Python\n",
    "// consistency_check = df.filter(col(\"salary\") + col(\"bonus\") != col(\"compTotal\"))\n",
    "\n",
    "consistencyCheck.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2a762",
   "metadata": {},
   "source": [
    "### Cardinality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6ab0da6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  1000|\n",
      "|  2000|\n",
      "|  3000|\n",
      "|  4000|\n",
      "|  5000|\n",
      "|  null|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cardinalityCheck: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [salary: int]\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Check cardinality of columns to understand how unique data is\n",
    "val cardinalityCheck = df.select(\"salary\").distinct()\n",
    "\n",
    "// Python\n",
    "// cardinality_check = df.select(\"salary\").distinct()\n",
    "\n",
    "cardinalityCheck.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30858e62",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8e5e755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies in age: 0\n",
      "Number of anomalies in salary: 0\n",
      "Number of anomalies in bonus: 0\n",
      "Number of anomalies in compTotal: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numericColumns: Seq[String] = List(age, salary, bonus, compTotal)\n",
       "statExprs: Seq[org.apache.spark.sql.Column] = List(avg(age) AS age_mean, stddev_samp(age) AS age_stddev, avg(salary) AS salary_mean, stddev_samp(salary) AS salary_stddev, avg(bonus) AS bonus_mean, stddev_samp(bonus) AS bonus_stddev, avg(compTotal) AS compTotal_mean, stddev_samp(compTotal) AS compTotal_stddev)\n",
       "dfStats: org.apache.spark.sql.Row = [23.333333333333332,1.632993161855452,3333.3333333333335,1632.993161855452,350.0,361.93922141707714,3883.3333333333335,1665.4328766620006]\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Identify statistical anomalies or outliers in numeric columns\n",
    "val numericColumns = Seq(\"age\", \"salary\", \"bonus\", \"compTotal\")\n",
    "\n",
    "// Create a sequence of column expressions for avg and stddev\n",
    "val statExprs = numericColumns.flatMap(c =>\n",
    "    Seq(avg(col(c)).alias(s\"${c}_mean\"), stddev(col(c)).alias(s\"${c}_stddev\"))\n",
    ")\n",
    "\n",
    "// Collect the statistics\n",
    "val dfStats = df.select(statExprs: _*).collect()(0)\n",
    "\n",
    "// Iterate through each numeric column and check for anomalies\n",
    "numericColumns.foreach { column =>\n",
    "    val mean = dfStats.getAs[Double](s\"${column}_mean\")\n",
    "    val stddev = dfStats.getAs[Double](s\"${column}_stddev\")\n",
    "\n",
    "  // Anomaly Check\n",
    "  val anomalyCheck = df.filter((col(column) < (mean - 3 * stddev)) || (col(column) > (mean + 3 * stddev)))\n",
    "    .count()\n",
    "\n",
    "    println(s\"Number of anomalies in $column: $anomalyCheck\")\n",
    "}\n",
    "\n",
    "// Python\n",
    "// numeric_columns = [\"age\", \"salary\", \"bonus\", \"compTotal\"]\n",
    "\n",
    "// stat_exprs = []\n",
    "// for col in numeric_columns:\n",
    "//     stat_exprs.append(F.avg(F.col(col)).alias(f\"{col}_mean\"))\n",
    "//     stat_exprs.append(F.stddev(F.col(col)).alias(f\"{col}_stddev\"))\n",
    "\n",
    "// df_stats = df.select(*stat_exprs).collect()[0]\n",
    "\n",
    "// for col in numeric_columns:\n",
    "//     mean = df_stats[f\"{col}_mean\"]\n",
    "//     stddev = df_stats[f\"{col}_stddev\"]\n",
    "    \n",
    "//     anomaly_check = df.filter((F.col(col) < (mean - 3 * stddev)) | (F.col(col) > (mean + 3 * stddev))).count()\n",
    "\n",
    "//     print(f\"Number of anomalies in {col}: {anomaly_check}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
